{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyabf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Load the ABF file\n",
    "# ---------------------------\n",
    "\n",
    "abf = pyabf.ABF(\"PATH/TO/FILES/filename.abf\")\n",
    "\n",
    "signal = abf.sweepY   # Ionic current (e.g., in pA)\n",
    "time_signal = abf.sweepX   # Time vector (e.g., in seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sampling Rate:\", abf.dataRate, \"Hz\")\n",
    "\n",
    "print(\"Number of Sweeps:\", abf.sweepCount)\n",
    "\n",
    "print(\"Protocol Name:\", abf.protocol)\n",
    "\n",
    "print(\"Recording Start Time:\", abf.abfDateTime)\n",
    "\n",
    "print(\"Channel Units:\", abf.adcUnits)\n",
    "\n",
    "print(\"Channel Scaling Factors:\", abf.adcUnits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyabf.tools.abfHeaderDisplay import abfInfoPage\n",
    "\n",
    "info_page = abfInfoPage(abf)\n",
    "header_text = info_page.getText()  # This might throw the same error, but worth trying.\n",
    "print(header_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Plot the ionic current trace for single sweep\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(time_signal, signal, label=\"Ionic current\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Current (pA)\")\n",
    "plt.title(\"Ionic Current Trace\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Sweep Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sweeps = []\n",
    "all_times = []\n",
    "for sweep in range(1):\n",
    "    abf.setSweep(sweep)\n",
    "    all_sweeps.append(abf.sweepY)\n",
    "    all_times.append(abf.sweepX)\n",
    "\n",
    "data_all = np.concatenate(all_sweeps)\n",
    "time_all = np.concatenate(all_times)\n",
    "\n",
    "#sweeps = data\n",
    "sample_rate = abf.dataRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 35\n",
    "end_time = time_all[-1] - 50\n",
    "mask = (time_all >= start_time) & (time_all <= end_time)\n",
    "\n",
    "data = data_all[mask]\n",
    "time = time_all[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = data.reshape(-1, 1)\n",
    "print(\"Length of data\", len(data))\n",
    "n_clusters = 2\n",
    "\n",
    "# Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(data_c)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get the cluster centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(centroids)\n",
    "\n",
    "# Get points in each cluster\n",
    "cluster_1 = data_c[labels == 0]\n",
    "cluster_2 = data_c[labels == 1]\n",
    "# plot_cluster_histogram(cluster_1, cluster_2)\n",
    "cluster1_indices = np.where(kmeans.labels_ == 0)[0]\n",
    "cluster2_indices = np.where(kmeans.labels_ == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_flat = np.array([val[0] for val in cluster_1])\n",
    "cluster_2_flat = np.array([val[0] for val in cluster_2])\n",
    "plt.figure()\n",
    "plt.hist(cluster_1_flat, bins=100, color='blue', alpha=0.7, label='Cluster 1')\n",
    "plt.hist(cluster_2_flat, bins=100, color='red', alpha=0.7, label='Cluster 2')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cluster 1 and Cluster 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive baseline correction using rolling median\n",
    "window_duration_sec = 10  # Window size (seconds)\n",
    "window_size = int(window_duration_sec * sample_rate)  # Window size (samples)\n",
    "\n",
    "data_series = pd.Series(data)\n",
    "adaptive_baseline = data_series.rolling(window=window_size, center=True, min_periods=1).median()\n",
    "\n",
    "corrected_data = data - adaptive_baseline.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Define threshold and event detection parameters\n",
    "# -------------------------------\n",
    "\n",
    "# Set threshold (after baseline correction)\n",
    "threshold = np.median(corrected_data) - 1.5 * np.std(corrected_data)  # Adaptive threshold\n",
    "\n",
    "# Minimum event duration (e.g., 0.1 ms = 0.0001 s)\n",
    "min_event_duration = 0.0001\n",
    "min_event_points = int(min_event_duration * sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(time, data, label=\"Original Current\", color='blue', alpha=0.5)\n",
    "plt.plot(time, adaptive_baseline, label=\"Adaptive Baseline\", color='orange', linewidth=2)\n",
    "adaptive_threshold = adaptive_baseline + threshold\n",
    "plt.plot(time, adaptive_threshold, color='red', linestyle='--', label=\"Adaptive Threshold\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Current (pA)\")\n",
    "plt.title(\"Ionic Current Trace with Adaptive Baseline and Adaptive Threshold\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect indices where current is below the adaptive threshold\n",
    "below_threshold = np.where(data < adaptive_threshold)[0]\n",
    "\n",
    "if len(below_threshold) == 0:\n",
    "    print(\"No events detected with the current threshold.\")\n",
    "    exit()\n",
    "\n",
    "# Group contiguous indices into individual events\n",
    "def group_contiguous(indices, max_gap=100):\n",
    "    \"\"\"\n",
    "    Group contiguous indices into clusters if consecutive indices\n",
    "    are separated by no more than max_gap samples.\n",
    "    \"\"\"\n",
    "    if len(indices) == 0:\n",
    "        return []\n",
    "    \n",
    "    groups = []\n",
    "    current_group = [indices[0]]\n",
    "    \n",
    "    for i in range(1, len(indices)):\n",
    "        if indices[i] - indices[i - 1] <= max_gap:\n",
    "            current_group.append(indices[i])\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [indices[i]]\n",
    "    \n",
    "    groups.append(current_group)\n",
    "    return groups\n",
    "\n",
    "# Group indices below the threshold into events\n",
    "event_groups = group_contiguous(below_threshold, max_gap=10)\n",
    "\n",
    "# Filter out events that are too short to be considered valid\n",
    "event_groups = [group for group in event_groups if len(group) >= min_event_points]\n",
    "\n",
    "# Print the number of detected events\n",
    "print(f\"Detected {len(event_groups)} events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract event properties\n",
    "events = []\n",
    "for group in event_groups:\n",
    "    start_idx = group[0]\n",
    "    end_idx = group[-1]\n",
    "    event_time = time[start_idx:end_idx + 1]\n",
    "    event_data = data[start_idx:end_idx + 1]\n",
    "    \n",
    "    duration = event_time[-1] - event_time[0]\n",
    "    amplitude = np.min(event_data)  # Minimum current during the event\n",
    "    \n",
    "    events.append({\n",
    "        \"start_time\": event_time[0],\n",
    "        \"end_time\": event_time[-1],\n",
    "        \"duration\": duration,\n",
    "        \"amplitude\": amplitude\n",
    "    })\n",
    "\n",
    "# Print detected events\n",
    "print(\"Detected events:\")\n",
    "for i, event in enumerate(events, start=1):\n",
    "    print(f\"Event {i}: Start = {event['start_time']:.3f} s, \"\n",
    "          f\"End = {event['end_time']:.3f} s, Duration = {event['duration']:.3f} s, \"\n",
    "          f\"Amplitude = {event['amplitude']:.2f} pA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list to store event information as dictionaries\n",
    "event_list = []\n",
    "for i, group in enumerate(event_groups, start=1):\n",
    "    # Get the indices for this event\n",
    "    extra_points = 300\n",
    "    start_idx = group[0] - extra_points\n",
    "    end_idx = group[-1] + extra_points\n",
    "    \n",
    "    # Extract the time and current for this event and convert to lists\n",
    "    event_time = time[start_idx:end_idx+1].tolist()\n",
    "    event_current = data[start_idx:end_idx+1].tolist()\n",
    "    \n",
    "    # Calculate duration and amplitude (minimum current)\n",
    "    duration = event_time[-1] - event_time[0]\n",
    "    amplitude = min(event_current)\n",
    "    \n",
    "    # Append the event details to the list\n",
    "    event_list.append({\n",
    "        \"event_index\": i,\n",
    "        \"start_time\": event_time[0],\n",
    "        \"end_time\": event_time[-1],\n",
    "        \"duration\": duration,\n",
    "        \"amplitude\": amplitude,\n",
    "        \"time_series\": event_time,\n",
    "        \"current_series\": event_current\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df_events = pd.DataFrame(event_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_events = pd.read_csv(\"detected_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_blockade_percentage(event):\n",
    "    current_series = event[\"current_series\"]\n",
    "    if len(current_series) < 10:\n",
    "        return 0\n",
    "    baseline = np.median(current_series[:50]) if len(current_series) > 50 else np.median(current_series)\n",
    "    min_current = min(current_series)\n",
    "    blockade_depth = baseline - min_current\n",
    "    if baseline == 0:\n",
    "        return 0\n",
    "    return (blockade_depth / baseline) * 100\n",
    "\n",
    "# blockade_percentage to each event\n",
    "for event in event_list:\n",
    "    event[\"blockade_percentage\"] = compute_blockade_percentage(event)\n",
    "\n",
    "df_events = pd.DataFrame(event_list)\n",
    "\n",
    "# Filtering events with blockade_percentage >= 5%\n",
    "filtered_events_df = df_events[df_events[\"blockade_percentage\"] >= 5].reset_index(drop=True)\n",
    "\n",
    "# filtered DataFrame\n",
    "filtered_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.widgets as widgets\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def plot_events_interactive_jupyter(filtered_events_df):\n",
    "\n",
    "    try:\n",
    "        from ipywidgets import interact, IntSlider, Dropdown\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if filtered_events_df.empty:\n",
    "            print(\"No events to plot.\")\n",
    "            return\n",
    "        \n",
    "        # Get only the event indices that exist in filtered_events_df\n",
    "        available_event_indices = sorted(filtered_events_df[\"event_index\"].tolist())\n",
    "        \n",
    "        def plot_single_event(slider_position):\n",
    "            \"\"\"Plot a single event based on the slider position.\"\"\"\n",
    "            if slider_position < 0 or slider_position >= len(available_event_indices):\n",
    "                print(f\"Invalid slider position: {slider_position}\")\n",
    "                return\n",
    "                \n",
    "            event_idx = available_event_indices[slider_position]\n",
    "            \n",
    "            event_row = filtered_events_df[filtered_events_df[\"event_index\"] == event_idx]\n",
    "            if event_row.empty:\n",
    "                print(f\"Event index {event_idx} not found in filtered_events_df.\")\n",
    "                return\n",
    "            \n",
    "            event_row = event_row.iloc[0]\n",
    "            time_series = event_row[\"time_series\"]\n",
    "            current_series = event_row[\"current_series\"]\n",
    "            start_time = event_row[\"start_time\"]\n",
    "            end_time = event_row[\"end_time\"]\n",
    "            blockade_percentage = event_row.get(\"blockade_percentage\", 0)\n",
    "            \n",
    "            if isinstance(time_series, str):\n",
    "                time_series = ast.literal_eval(time_series)\n",
    "            if isinstance(current_series, str):\n",
    "                current_series = ast.literal_eval(current_series)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(time_series, current_series, label=f\"Event {event_idx}\", color=\"blue\", linewidth=1.5)\n",
    "            \n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Current (A)\")\n",
    "            plt.title(f\"Event {event_idx} ({slider_position+1}/{len(available_event_indices)}) - Blockade: {blockade_percentage:.1f}% (Time {start_time:.3f} to {end_time:.3f} s)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        interact(plot_single_event, \n",
    "                slider_position=IntSlider(\n",
    "                    value=0,\n",
    "                    min=0,\n",
    "                    max=len(available_event_indices)-1,\n",
    "                    step=1,\n",
    "                    description='Event:',\n",
    "                    continuous_update=False,  # Only update when slider is released\n",
    "                    layout={'width': '400px'}\n",
    "                ))\n",
    "        \n",
    "        print(f\"Use the slider to navigate between {len(available_event_indices)} available events\")\n",
    "        print(f\"Event indices range: {available_event_indices[0]} to {available_event_indices[-1]}\")\n",
    "        print(f\"Available event indices: {available_event_indices[:10]}{'...' if len(available_event_indices) > 10 else ''}\")\n",
    "        \n",
    "    except ImportError:\n",
    "\n",
    "def plot_single_event_manual(filtered_events_df, event_idx):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import ast\n",
    "    \n",
    "    event_row = filtered_events_df[filtered_events_df[\"event_index\"] == event_idx]\n",
    "    if event_row.empty:\n",
    "        print(f\"Event index {event_idx} not found in filtered_events_df.\")\n",
    "        available_indices = sorted(filtered_events_df[\"event_index\"].tolist())\n",
    "        print(f\"Available event indices: {available_indices}\")\n",
    "        return\n",
    "    \n",
    "    event_row = event_row.iloc[0]\n",
    "    time_series = event_row[\"time_series\"]\n",
    "    current_series = event_row[\"current_series\"]\n",
    "    start_time = event_row[\"start_time\"]\n",
    "    end_time = event_row[\"end_time\"]\n",
    "    blockade_percentage = event_row.get(\"blockade_percentage\", 0)\n",
    "    \n",
    "    if isinstance(time_series, str):\n",
    "        time_series = ast.literal_eval(time_series)\n",
    "    if isinstance(current_series, str):\n",
    "        current_series = ast.literal_eval(current_series)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time_series, current_series, label=f\"Event {event_idx}\", color=\"blue\", linewidth=1.5)\n",
    "    \n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Current (A)\")\n",
    "    plt.title(f\"Event {event_idx} - Blockade: {blockade_percentage:.1f}% (Time {start_time:.3f} to {end_time:.3f} s)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_events_interactive_jupyter(filtered_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def estimate_levels_gmm(signal, max_states=6):\n",
    "    X = signal.reshape(-1, 1)\n",
    "    best_bic = np.inf\n",
    "    best_k = 1\n",
    "    best_gmm = None\n",
    "    \n",
    "    for k in range(1, max_states + 1):\n",
    "        gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=0)\n",
    "        gmm.fit(X)\n",
    "        bic = gmm.bic(X)\n",
    "        if bic < best_bic:\n",
    "            best_bic = bic\n",
    "            best_k = k\n",
    "            best_gmm = gmm\n",
    "    \n",
    "    level_values = np.sort(best_gmm.means_.flatten())\n",
    "    return best_k, level_values\n",
    "\n",
    "for event in event_list:\n",
    "    event_time = np.array(event[\"time_series\"])\n",
    "    event_current = np.array(event[\"current_series\"])\n",
    "    \n",
    "    if len(event_current) > 0:\n",
    "        baseline = event_current[0]\n",
    "        mean_current = event_current.mean()\n",
    "        std_current = event_current.std()\n",
    "        area = np.trapz(event_current, event_time)\n",
    "        peak_current = event_current.max()\n",
    "        \n",
    "        num_level, level_values = estimate_levels_gmm(event_current, max_states=8)\n",
    "        volatility = std_current / np.abs(mean_current) if mean_current != 0 else np.nan\n",
    "    else:\n",
    "        baseline = mean_current = std_current = area = peak_current = np.nan\n",
    "        num_level = np.nan\n",
    "        level_values = []\n",
    "        volatility = np.nan\n",
    "\n",
    "    if len(event_current) > 1:\n",
    "        slopes = np.gradient(event_current, event_time)\n",
    "        max_slope = slopes.max()\n",
    "        min_slope = slopes.min()\n",
    "    else:\n",
    "        max_slope = min_slope = np.nan\n",
    "    \n",
    "    n_points = len(event_current)\n",
    "    \n",
    "    event.update({\n",
    "        \"baseline\": baseline,\n",
    "        \"mean_current\": mean_current,\n",
    "        \"std_current\": std_current,\n",
    "        \"area\": area,\n",
    "        \"peak_current\": peak_current,\n",
    "        \"max_slope\": max_slope,\n",
    "        \"min_slope\": min_slope,\n",
    "        \"n_points\": n_points,\n",
    "        \"num_level\": num_level,\n",
    "        \"level_values\": level_values.tolist(),  # Convert numpy array to list for storage\n",
    "        \"Noise\": volatility\n",
    "    })\n",
    "\n",
    "# Build DataFrame\n",
    "filtered_events_df = pd.DataFrame(event_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events_df.to_csv(\"detected_events_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events_df.to_parquet('Lamdaa_with_features.parquet', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
